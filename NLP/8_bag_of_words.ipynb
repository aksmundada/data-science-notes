{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words in NLP\n",
    "\n",
    "### Context\n",
    "The Bag of Words (BoW) model is a fundamental technique in Natural Language Processing (NLP) for representing text data. It converts text into numerical feature vectors based on word frequency, enabling machine learning models to process textual information.\n",
    "\n",
    "#### Key Points:\n",
    "- **Purpose**: Represents text data as a fixed-size vector of word frequencies.\n",
    "- **Usage**:\n",
    "  - Commonly used in text classification, sentiment analysis, and information retrieval tasks.\n",
    "  - Provides a simple and effective method for feature extraction.\n",
    "- **How It Works**:\n",
    "  - A vocabulary of unique words is created from the text corpus.\n",
    "  - Each document is represented as a vector where each element corresponds to the frequency of a word in the document.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's implement the Bag of Words model for a small example corpus in Python. We will demonstrate both the standard BoW (using word frequencies) and Binary BoW (indicating word presence or absence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard BoW Vocabulary: ['hello' 'learning' 'machine' 'world']\n",
      "\n",
      "Standard BoW Feature Matrix:\n",
      "[[2 0 0 1]\n",
      " [0 1 1 0]\n",
      " [1 0 1 0]]\n",
      "\n",
      "Binary BoW Vocabulary: ['hello' 'learning' 'machine' 'world']\n",
      "\n",
      "Binary BoW Feature Matrix:\n",
      "[[1 0 0 1]\n",
      " [0 1 1 0]\n",
      " [1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"hello world hello\",\n",
    "    \"machine learning\",\n",
    "    \"hello machine\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize the CountVectorizer for BoW\n",
    "vectorizer_bow = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit the vectorizer to the corpus and transform it for standard BoW\n",
    "X_bow = vectorizer_bow.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Retrieve the vocabulary and feature matrix for BoW\n",
    "vocabulary_bow = vectorizer_bow.get_feature_names_out()\n",
    "feature_matrix_bow = X_bow.toarray()\n",
    "\n",
    "print(\"Standard BoW Vocabulary:\", vocabulary_bow)\n",
    "print(\"\\nStandard BoW Feature Matrix:\")\n",
    "print(feature_matrix_bow)\n",
    "\n",
    "# Step 4: Initialize the CountVectorizer for Binary BoW (binary=True)\n",
    "vectorizer_binary_bow = CountVectorizer(binary=True)\n",
    "\n",
    "# Step 5: Fit the vectorizer to the corpus and transform it for Binary BoW\n",
    "X_binary_bow = vectorizer_binary_bow.fit_transform(corpus)\n",
    "\n",
    "# Step 6: Retrieve the vocabulary and feature matrix for Binary BoW\n",
    "vocabulary_binary_bow = vectorizer_binary_bow.get_feature_names_out()\n",
    "feature_matrix_binary_bow = X_binary_bow.toarray()\n",
    "\n",
    "print(\"\\nBinary BoW Vocabulary:\", vocabulary_binary_bow)\n",
    "print(\"\\nBinary BoW Feature Matrix:\")\n",
    "print(feature_matrix_binary_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a toy dataset for spam classification using the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "'You have won a free gift! Claim now.' -> Spam\n",
      "'Let's meet for dinner tomorrow.' -> Not Spam\n",
      "'Congratulations, you are selected for an exclusive deal!' -> Spam\n",
      "'Can you send me the report by evening?' -> Spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample toy dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Win a free lottery now\",\n",
    "        \"Call now for exclusive offer\",\n",
    "        \"Hey, how are you doing today?\",\n",
    "        \"Congratulations! You have won a prize\",\n",
    "        \"Let's catch up for coffee\",\n",
    "        \"Cheap loans available, apply now\",\n",
    "        \"Are we still meeting tomorrow?\",\n",
    "        \"Earn money fast with this trick\",\n",
    "        \"Meeting rescheduled to next week\",\n",
    "        \"Exclusive deal just for you\"\n",
    "    ],\n",
    "    \"label\": [1, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 1: Spam, 0: Not Spam\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing text using BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bowb = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Training a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "\n",
    "# Evaluating model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting for new sentences\n",
    "new_sentences = [\n",
    "    \"You have won a free gift! Claim now.\",\n",
    "    \"Let's meet for dinner tomorrow.\",\n",
    "    \"Congratulations, you are selected for an exclusive deal!\"\n",
    "]\n",
    "\n",
    "# Transform new sentences using the trained vectorizer\n",
    "new_sentences_bow = vectorizer.transform(new_sentences)\n",
    "new_predictions = classifier.predict(new_sentences_bow)\n",
    "\n",
    "# Display predictions\n",
    "for sentence, prediction in zip(new_sentences, new_predictions):\n",
    "    label = \"Spam\" if prediction == 1 else \"Not Spam\"\n",
    "    print(f\"'{sentence}' -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Difference\n",
    "- **Standard BoW** captures word frequencies, which can help determine the importance of a word in a document.\n",
    "- **Binary BoW** focuses solely on word presence, ignoring frequency, making it useful when word occurrence is more important than count.\n",
    "\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "| **Advantages**                                | **Limitations**                                                                 |\n",
    "|-----------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| Simple to implement and understand            | Ignores word order and syntactic structure                                     |\n",
    "| Effective for small to medium-sized datasets  | Results in sparse and high-dimensional matrices for large vocabularies         |\n",
    "| Compatible with most machine learning models  | Does not capture semantic meaning or relationships between words               |\n",
    "| Allows for straightforward feature extraction | Out of Vocabulary (OOV) Issue leading to loss of information.                  |\n",
    "\n",
    "### Conclusion\n",
    "The Bag of Words model is a foundational NLP technique that transforms text data into numerical form based on word frequency. While it is simple and effective for certain applications, its limitations, such as ignoring word order and semantics, make it less suitable for complex tasks. More advanced methods like TF-IDF, Word2Vec, and BERT address these issues by incorporating context and semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

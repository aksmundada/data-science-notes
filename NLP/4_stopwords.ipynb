{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Stopwords\n",
    "\n",
    "### 1. **Context**\n",
    "Text preprocessing is a crucial step in Natural Language Processing (NLP), and **stopwords removal** plays an essential role in improving the efficiency of NLP models. **Stopwords** are common words like \"the,\" \"and,\" \"is,\" \"in,\" etc., that don't add much meaningful content to the text. These words are often removed during text processing to reduce the size of the dataset and improve the performance of models.\n",
    "\n",
    "In this notebook, we will focus on **stopwords removal** using the **NLTK (Natural Language Toolkit)** library, which provides a powerful set of tools for text analysis and preprocessing tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Install NLTK**\n",
    "Before starting, make sure that NLTK is installed on your system.\n",
    "\n",
    "```bash\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download NLTK Stopwords\n",
    "To use NLTKâ€™s stopwords, we need to download the corresponding corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\IT\n",
      "[nltk_data]     Support\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example of Stopwords in Text\n",
    "Let's take a look at an example sentence and identify the stopwords in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, ['i', 'me', 'my', 'myself', 'we'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of English stopwords and display the first 5 stopwords\n",
    "len(stopwords.words('english')), stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This is a simple sentence, and it contains stopwords.\n",
      "Filtered Text:  simple sentence , contains stopwords .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple sentence, and it contains stopwords.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Filter out stopwords from the tokenized words\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(\"Original Text: \", text)\n",
    "print(\"Filtered Text: \", \" \".join(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explanation of the Code\n",
    "* We use **NLTK's** *stopwords* corpus to access the list of common stopwords in English.\n",
    "* **Tokenization**: The *word_tokenize* function splits the text into individual words.\n",
    "* **Stopwords Removal**: A list comprehension is used to filter out words from the tokenized list that are present in the stopwords set.\n",
    "\n",
    "### 6. Custom Stopwords List\n",
    "Sometimes, we may need to create our own custom list of stopwords to remove specific words related to a particular domain or task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This is a simple sentence, and it contains stopwords.\n",
      "Custom Filtered Text:  this is a simple , and it stopwords .\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = {\"sentence\", \"contains\"}\n",
    "\n",
    "# Filter out custom stopwords\n",
    "custom_filtered_words = [word for word in words if word not in custom_stopwords]\n",
    "\n",
    "print(\"Original Text: \", text)\n",
    "print(\"Custom Filtered Text: \", \" \".join(custom_filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Handling Punctuation and Special Characters\n",
    "It's common to remove punctuation and special characters along with stopwords. Here's how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This is a simple sentence, and it contains stopwords.\n",
      "Text without Punctuation:  simple sentence contains stopwords\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuation from the words\n",
    "words_no_punct = [word for word in filtered_words if word not in string.punctuation]\n",
    "\n",
    "print(\"Original Text: \", text)\n",
    "print(\"Text without Punctuation: \", \" \".join(words_no_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stopwords Removal for Larger Texts\n",
    "For longer texts or larger datasets, stopwords removal can be performed in bulk using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Large Text:  example much larger text multiple sentences easily remove stopwords\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_from_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Example large text\n",
    "large_text = \"This is an example of a much larger text with multiple sentences. We can easily remove stopwords from it.\"\n",
    "\n",
    "processed_text = remove_stopwords_from_text(large_text)\n",
    "print(\"Processed Large Text: \", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "\n",
    "Stopwords removal is a critical part of text preprocessing that helps in cleaning and reducing the noise in textual data. The NLTK library provides an easy-to-use set of tools for stopwords removal, which is essential for enhancing the performance of many NLP models.\n",
    "\n",
    "#### Key Takeaways:\n",
    "- **Stopwords** are common words that don't contribute much to the meaning of the text.\n",
    "- **NLTK** provides a ready-made list of stopwords for various languages.\n",
    "- **Stopwords removal** helps in reducing the dimensionality of the data, leading to more efficient models.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Further Enhancements\n",
    "\n",
    "While removing stopwords is useful, sometimes it might not be necessary in all cases. Here are a few advanced steps you can explore:\n",
    "\n",
    "- **Domain-specific stopwords**: Build a custom stopwords list specific to your domain.\n",
    "- **Lemmatization or Stemming**: Combine stopwords removal with lemmatization or stemming for more effective text normalization.\n",
    "- **Handling Different Languages**: Use NLTK's support for multiple languages to remove stopwords in languages other than English."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding in NLP\n",
    "\n",
    "### Context\n",
    "One-hot encoding is a fundamental technique in Natural Language Processing (NLP) used to represent categorical data, such as words or characters, in a numerical format.\n",
    "\n",
    "#### Key Points:\n",
    "- **Purpose**: Converts categorical data (e.g., words, characters) into numerical representations.\n",
    "- **Usage**:\n",
    "  - Often used as a preprocessing step in NLP tasks.\n",
    "  - Prepares text data for input into machine learning models.\n",
    "- **How It Works**:\n",
    "  - Each word/token is represented as a vector.\n",
    "  - All elements in the vector are set to `0`, except for one element corresponding to the word's index, which is set to `1`.\n",
    "\n",
    "This method provides a foundational understanding for transitioning to more advanced embedding techniques like Word2Vec or GloVe.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's implement one-hot encoding for a small example corpus in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-to-Index Mapping: {'machine': 0, 'learning': 1, 'world': 2, 'hello': 3}\n",
      "\n",
      "One-Hot Encodings:\n",
      "machine: [1. 0. 0. 0.]\n",
      "learning: [0. 1. 0. 0.]\n",
      "world: [0. 0. 1. 0.]\n",
      "hello: [0. 0. 0. 1.]\n",
      "\n",
      "Encoded Sentence:\n",
      "hello: [0. 0. 0. 1.]\n",
      "world: [0. 0. 1. 0.]\n",
      "\n",
      "Shape of the encoded sentence: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\"hello world\", \"machine learning\", \"hello machine\"]\n",
    "\n",
    "# Step 1: Tokenize the corpus into individual words\n",
    "tokens = set(word for sentence in corpus for word in sentence.split())\n",
    "\n",
    "# Step 2: Create a word-to-index mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(tokens)}\n",
    "print(\"Word-to-Index Mapping:\", word_to_index)\n",
    "\n",
    "# Step 3: Generate one-hot encodings\n",
    "vocab_size = len(tokens)\n",
    "one_hot_encodings = {}\n",
    "\n",
    "for word, idx in word_to_index.items():\n",
    "    encoding = np.zeros(vocab_size)\n",
    "    encoding[idx] = 1\n",
    "    one_hot_encodings[word] = encoding\n",
    "\n",
    "print(\"\\nOne-Hot Encodings:\")\n",
    "for word, encoding in one_hot_encodings.items():\n",
    "    print(f\"{word}: {encoding}\")\n",
    "\n",
    "# Encoding an example sentence\n",
    "sentence = \"hello world\"\n",
    "encoded_sentence = [one_hot_encodings[word] for word in sentence.split()]\n",
    "print(\"\\nEncoded Sentence:\")\n",
    "for word, encoding in zip(sentence.split(), encoded_sentence):\n",
    "    print(f\"{word}: {encoding}\")\n",
    "\n",
    "# Shape of the encoded sentence\n",
    "encoded_sentence_array = np.array(encoded_sentence)\n",
    "print(\"\\nShape of the encoded sentence:\", encoded_sentence_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Shape\n",
    "- The shape of the encoded sentence will be `(n, m)`, where:\n",
    "  - `n` is the number of words in the sentence.\n",
    "  - `m` is the size of the vocabulary (number of unique words in the corpus).\n",
    "- Each row in the encoded sentence represents a one-hot encoded vector for a single word.\n",
    "- For example, if the sentence \"hello world\" has 2 words and the vocabulary size is 4, the shape of the encoded sentence will be `(2, 4)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages and Limitations\n",
    "\n",
    "| **Advantages**                                | **Limitations**                                                                 |\n",
    "|-----------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| Simple and easy to implement                  | High dimensionality for large vocabularies                                     |\n",
    "| Effective for small vocabularies              | Does not capture semantic relationships between words                          |\n",
    "| Provides a foundation for understanding more advanced techniques | Inefficient representation for datasets with a large number of unique categories |\n",
    "| Interpretable and deterministic representation| Sparse matrices lead to memory inefficiency                                    |\n",
    "| Useful for tasks requiring clear separation of categories | Unable to handle unseen words or categories during inference                   |\n",
    "| No computation required beyond basic indexing | No inherent ability to represent word similarity or context  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Conclusion\n",
    "One-hot encoding is a simple yet powerful technique for converting categorical text data into numerical form, facilitating its use in machine learning models. While effective for small datasets, it has limitations such as high dimensionality and lack of semantic meaning. Advanced methods like word embeddings (e.g., Word2Vec, GloVe) address these limitations by capturing semantic relationships between words in a dense, continuous vector space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

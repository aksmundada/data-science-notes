{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF in NLP\n",
    "\n",
    "### Context\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used in Natural Language Processing (NLP) to evaluate the importance of a word in a document relative to a collection of documents (corpus). Unlike Bag of Words (BoW), which considers raw frequencies, TF-IDF assigns higher weights to words that are important within a document but rare across the corpus.\n",
    "\n",
    "#### Key Points:\n",
    "- **Purpose**: Helps distinguish important words in a document while reducing the influence of common words.\n",
    "- **Usage**:\n",
    "  - Commonly used in text classification, information retrieval, and search engines.\n",
    "  - Improves upon BoW by reducing the weight of frequently occurring but unimportant words.\n",
    "- **Mathematical Formulation**:\n",
    "  - **Term Frequency (TF)**: Measures how often a word appears in a document.\n",
    "    \n",
    "    $TF(w) = \\frac{\\text{Number of times } w \\text{ appears in the document}}{\\text{Total number of words in the document}}$\n",
    "\n",
    "\n",
    "  \n",
    "  - **Inverse Document Frequency (IDF)**: Measures how unique a word is across the corpus.\n",
    "    \n",
    "    $IDF(w) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing } w} \\right)$\n",
    "  \n",
    "  - **TF-IDF Score**: The final importance score of a word.\n",
    "    \n",
    "    $TF\\text{-}IDF(w) = TF(w) \\times IDF(w)$\n",
    "\n",
    "### Example: Text Classification using TF-IDF\n",
    "\n",
    "Let's use a toy dataset to demonstrate how TF-IDF works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Accuracy: 0.25\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n",
      "'I loved the film and the story was amazing' -> Positive\n",
      "'Worst experience, the plot was awful' -> Negative\n",
      "'The cinematography and acting were fantastic' -> Positive\n",
      "'Horrible script with bad direction' -> Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Expanded dataset to improve learning\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"The movie was fantastic and I loved it\",\n",
    "        \"Absolutely terrible movie, I hated it\",\n",
    "        \"Great film with an amazing story\",\n",
    "        \"The story was dull and boring\",\n",
    "        \"A fantastic experience with great actors\",\n",
    "        \"Worst film ever, do not watch it\",\n",
    "        \"Loved the characters and the direction\",\n",
    "        \"Terrible plot and bad acting\",\n",
    "        \"The acting was great and the film was entertaining\",\n",
    "        \"The movie was slow and had a bad script\",\n",
    "        \"Best movie ever, highly recommended!\",\n",
    "        \"Disappointing film with a predictable storyline\",\n",
    "        \"Brilliant performances, a must-watch!\",\n",
    "        \"Avoid this movie, it's a waste of time\",\n",
    "        \"Spectacular direction and cinematography\",\n",
    "        \"Horrible movie with no redeeming qualities\",\n",
    "        \"The screenplay and performances were top-notch\",\n",
    "        \"The pacing was terrible, made me fall asleep\"\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting dataset (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing text using TF-IDF with tuning\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))  # Unigrams & bigrams\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Training a Naive Bayes classifier with smoothing\n",
    "classifier = MultinomialNB(alpha=0.5)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluating model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"TF-IDF Model Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Predicting for new sentences\n",
    "new_sentences = [\n",
    "    \"I loved the film and the story was amazing\",\n",
    "    \"Worst experience, the plot was awful\",\n",
    "    \"The cinematography and acting were fantastic\",\n",
    "    \"Horrible script with bad direction\"\n",
    "]\n",
    "\n",
    "# Transform new sentences using the trained vectorizer\n",
    "new_sentences_tfidf = tfidf_vectorizer.transform(new_sentences)\n",
    "new_predictions = classifier.predict(new_sentences_tfidf)\n",
    "\n",
    "# Display predictions\n",
    "for sentence, prediction in zip(new_sentences, new_predictions):\n",
    "    label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"'{sentence}' -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Explaining TF-IDF with Logarithm  \n",
    "\n",
    "### üèõ Imagine a Library  \n",
    "- You have a big **library** full of books.  \n",
    "- Some words, like **\"the\"**, **\"is\"**, and **\"a\"**, appear in **almost every book**.  \n",
    "- Other words, like **\"volcano\"**, **\"tornado\"**, or **\"dinosaur\"**, appear in **only a few books**.  \n",
    "\n",
    "Now, you want to find **which words are important in each book**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Step 1: Count the Words (TF - Term Frequency)  \n",
    "You look at a book and **count** how many times a word appears.  \n",
    "\n",
    "- If **\"dinosaur\"** appears **10 times**, its **TF is 10**.  \n",
    "- If **\"the\"** appears **100 times**, its **TF is 100**.  \n",
    "\n",
    "But is **\"the\"** more important than **\"dinosaur\"**? ü§î **No!**  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Step 2: Find Out How Common the Word Is (IDF - Inverse Document Frequency)  \n",
    "- If **\"dinosaur\"** appears in **only 2 books out of 1,000**, it's **rare** and **important**! ü¶ñ  \n",
    "- If **\"the\"** appears in **every single book**, it's **too common** and **not important**.  \n",
    "\n",
    "So, we give **rare words a higher score** and **common words a lower score**.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§Ø Step 3: Why Use Logarithm?  \n",
    "If we just divide, rare words might get **huge** scores, making things **unfair**.  \n",
    "\n",
    "#### ‚ùå Without Log  \n",
    "- \"dinosaur\" appears in **2 books** ‚Üí  \n",
    "  **IDF = 1000 √∑ 2 = 500**  \n",
    "- \"volcano\" appears in **10 books** ‚Üí  \n",
    "  **IDF = 1000 √∑ 10 = 100**  \n",
    "\n",
    "Whoa! üò≤ **That‚Äôs a HUGE difference!**  \n",
    "\n",
    "#### ‚úÖ With Log  \n",
    "- \"dinosaur\" IDF =  \n",
    "  $$ \\log \\left(\\frac{1000}{2}\\right) = \\log(500) \\approx 2.7 $$  \n",
    "- \"volcano\" IDF =  \n",
    "  $$ \\log \\left(\\frac{1000}{10}\\right) = \\log(100) \\approx 2 $$  \n",
    "\n",
    "Now, the scores are **closer and more balanced**! üëç  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Final Answer  \n",
    "üìå We use **logarithm** because it **smooths the difference** between **rare and common words**, making the importance **fair and balanced**. ‚öñÔ∏è  \n",
    "\n",
    "üöÄ So, **TF-IDF with log** helps computers **find the most important words in a book or article** without getting confused by common words!  \n",
    "\n",
    "---\n",
    "\n",
    "Hope that makes sense! Let me know if you want an even simpler version! üòä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Reduces the influence of commonly occurring words.\n",
    "- Helps in extracting meaningful words from text data.\n",
    "- Works well in text classification and information retrieval.\n",
    "\n",
    "### Disadvantages\n",
    "- Still does not capture semantic meaning.\n",
    "- High dimensionality for large corpora.\n",
    "- May not work well for very short texts.\n",
    "\n",
    "### Conclusion\n",
    "TF-IDF is a powerful technique for text representation that improves upon BoW by weighting words based on their importance. It is widely used in search engines, text classification, and recommendation systems. However, for more advanced NLP tasks, embeddings like Word2Vec and BERT provide deeper semantic understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

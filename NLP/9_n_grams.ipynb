{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams in NLP\n",
    "\n",
    "### Context\n",
    "N-Grams are a more advanced text representation technique in Natural Language Processing (NLP) that capture the sequential structure of words. Unlike Bag of Words (BoW), which disregards word order, N-Grams preserve context by considering a sequence of N words together.\n",
    "\n",
    "#### Key Points:\n",
    "- **Purpose**: Represents text data by capturing word sequences of length N.\n",
    "- **Usage**:\n",
    "  - Helps in text classification, speech recognition, and machine translation.\n",
    "  - Addresses BoW limitations by preserving some context.\n",
    "- **How It Works**:\n",
    "  - A vocabulary of N-word sequences (N-Grams) is created.\n",
    "  - Each document is represented as a vector based on the frequency of these N-Grams.\n",
    "\n",
    "### Example: Sentiment Classification using N-Grams\n",
    "\n",
    "Let's use a toy dataset to show how N-Grams improve classification where BoW fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.50         4\n",
      "   macro avg       0.67      0.67      0.50         4\n",
      "weighted avg       0.83      0.50      0.50         4\n",
      "\n",
      "'I do not like this movie' -> Negative\n",
      "'This movie is fantastic' -> Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Expanded dataset for better bigram learning\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I love this movie\", \n",
    "        \"This movie is terrible\", \n",
    "        \"I enjoyed the film a lot\", \n",
    "        \"The film was not good\", \n",
    "        \"Amazing storyline and great acting\", \n",
    "        \"I do not like this film\",\n",
    "        \"The movie was fantastic\", \n",
    "        \"Absolutely horrible experience\",\n",
    "        \"I really liked this movie\", \n",
    "        \"I hated this film\",\n",
    "        \"The acting was brilliant\", \n",
    "        \"Worst movie I have ever seen\", \n",
    "        \"The movie was not bad\", \n",
    "        \"I do not hate this movie\",\n",
    "        \"Fantastic direction and cinematography\",\n",
    "        \"The worst film I watched this year\",\n",
    "        \"I would highly recommend this movie\",\n",
    "        \"This was a complete waste of time\",\n",
    "        \"Such a delightful movie experience\",\n",
    "        \"I would never watch this film again\",\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]  \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing text using Unigrams + Bigrams (BoW)\n",
    "# - `ngram_range=(1, 2)` means we include both unigrams (single words) and bigrams (two consecutive words)\n",
    "# - Unigrams help capture individual word importance\n",
    "# - Bigrams help capture short phrases and context\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Training a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Predicting for new sentences\n",
    "new_sentences = [\n",
    "    \"I do not like this movie\",\n",
    "    \"This movie is fantastic\"\n",
    "]\n",
    "\n",
    "# Transform and classify\n",
    "new_sentences_vectorized = vectorizer.transform(new_sentences)\n",
    "new_predictions = classifier.predict(new_sentences_vectorized)\n",
    "\n",
    "# Display predictions\n",
    "for sentence, prediction in zip(new_sentences, new_predictions):\n",
    "    label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"'{sentence}' -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'really': 59,\n",
       " 'liked': 45,\n",
       " 'this': 72,\n",
       " 'movie': 48,\n",
       " 'really liked': 60,\n",
       " 'liked this': 46,\n",
       " 'this movie': 74,\n",
       " 'do': 17,\n",
       " 'not': 54,\n",
       " 'like': 43,\n",
       " 'film': 26,\n",
       " 'do not': 18,\n",
       " 'not like': 58,\n",
       " 'like this': 44,\n",
       " 'this film': 73,\n",
       " 'worst': 81,\n",
       " 'have': 37,\n",
       " 'ever': 21,\n",
       " 'seen': 63,\n",
       " 'worst movie': 82,\n",
       " 'movie have': 50,\n",
       " 'have ever': 38,\n",
       " 'ever seen': 22,\n",
       " 'the': 68,\n",
       " 'was': 75,\n",
       " 'good': 30,\n",
       " 'the film': 70,\n",
       " 'film was': 29,\n",
       " 'was not': 78,\n",
       " 'not good': 56,\n",
       " 'such': 66,\n",
       " 'delightful': 13,\n",
       " 'experience': 23,\n",
       " 'such delightful': 67,\n",
       " 'delightful movie': 14,\n",
       " 'movie experience': 49,\n",
       " 'would': 83,\n",
       " 'highly': 39,\n",
       " 'recommend': 61,\n",
       " 'would highly': 84,\n",
       " 'highly recommend': 40,\n",
       " 'recommend this': 62,\n",
       " 'hate': 33,\n",
       " 'not hate': 57,\n",
       " 'hate this': 34,\n",
       " 'enjoyed': 19,\n",
       " 'lot': 47,\n",
       " 'enjoyed the': 20,\n",
       " 'film lot': 28,\n",
       " 'hated': 35,\n",
       " 'hated this': 36,\n",
       " 'never': 52,\n",
       " 'watch': 79,\n",
       " 'again': 4,\n",
       " 'would never': 85,\n",
       " 'never watch': 53,\n",
       " 'watch this': 80,\n",
       " 'film again': 27,\n",
       " 'amazing': 5,\n",
       " 'storyline': 64,\n",
       " 'and': 7,\n",
       " 'great': 31,\n",
       " 'acting': 2,\n",
       " 'amazing storyline': 6,\n",
       " 'storyline and': 65,\n",
       " 'and great': 9,\n",
       " 'great acting': 32,\n",
       " 'bad': 10,\n",
       " 'the movie': 71,\n",
       " 'movie was': 51,\n",
       " 'not bad': 55,\n",
       " 'absolutely': 0,\n",
       " 'horrible': 41,\n",
       " 'absolutely horrible': 1,\n",
       " 'horrible experience': 42,\n",
       " 'brilliant': 11,\n",
       " 'the acting': 69,\n",
       " 'acting was': 3,\n",
       " 'was brilliant': 76,\n",
       " 'fantastic': 24,\n",
       " 'direction': 15,\n",
       " 'cinematography': 12,\n",
       " 'fantastic direction': 25,\n",
       " 'direction and': 16,\n",
       " 'and cinematography': 8,\n",
       " 'was fantastic': 77}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Captures context better than BoW.\n",
    "- Useful in distinguishing phrases like \"not good\" vs. \"good movie\".\n",
    "- Reduces the sparsity issue compared to simple BoW.\n",
    "\n",
    "### Disadvantages\n",
    "- Increases dimensionality with higher N values.\n",
    "- Computationally expensive for large datasets.\n",
    "- Still does not capture deep semantic meaning.\n",
    "\n",
    "### Conclusion\n",
    "N-Grams provide a balance between BoW and deep learning-based NLP models by preserving some word order information. While bigrams and trigrams help in capturing local context, more advanced models like word embeddings and transformers further enhance text understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📖 Word Embeddings Explained (Like You're 10 Years Old!)\n",
    "\n",
    "#### 🎭 Imagine Words as Characters in a Movie\n",
    "Think of each word as a character in a big Hollywood movie.\n",
    "Some characters are best friends, some are enemies, and some never meet.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Superman and Batman** → Similar (both superheroes 🦸‍♂️)\n",
    "- **Superman and Joker** → Not similar (hero vs. villain 😈)\n",
    "- **Superman and Pizza** → No real connection 🍕🤔\n",
    "\n",
    "Just like characters have relationships, words also have relationships!\n",
    "\n",
    "---\n",
    "\n",
    "#### 🏠 How Words Were Stored Before (Bag of Words)\n",
    "Before word embeddings, computers stored words in a big table with just numbers.\n",
    "\n",
    "###### Example:\n",
    "| Sentence | Movie | Bad | Amazing | Loved | Acting |\n",
    "|----------|-------|-----|---------|-------|--------|\n",
    "| \"The movie was amazing\" | 1 | 0 | 1 | 0 | 0 |\n",
    "| \"I loved the acting\" | 0 | 0 | 0 | 1 | 1 |\n",
    "\n",
    "###### 👎 Problems:\n",
    "- It doesn't understand meaning (e.g., \"good\" and \"amazing\" are similar but treated as different words).\n",
    "- It only counts words but doesn’t know if \"bad\" and \"terrible\" are related.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 How Word Embeddings Fix This\n",
    "\n",
    "###### 🌍 Words Live in a \"Word Universe\"\n",
    "Instead of just counting words, we place each word in a giant space (like a galaxy 🌌) where:\n",
    "\n",
    "- **Similar words are closer together** 🚀\n",
    "- **Different words are far apart** 🏝️\n",
    "\n",
    "###### Example:\n",
    "- 🟢 (\"King\" and \"Queen\") → Very close 👑\n",
    "- 🔵 (\"Dog\" and \"Cat\") → Also close 🐶🐱\n",
    "- 🔴 (\"King\" and \"Apple\") → Very far apart 👑🍎\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔢 Numbers Behind the Magic (Vector Representation)\n",
    "Each word is stored as a list of numbers (called a \"vector\").\n",
    "\n",
    "###### Example:\n",
    "- **\"King\"** → `[0.9, 0.8, 0.2, 0.1]`\n",
    "- **\"Queen\"** → `[0.91, 0.79, 0.21, 0.09]`\n",
    "- **\"Apple\"** → `[0.2, 0.1, 0.9, 0.8]`\n",
    "\n",
    "👀 Notice how **\"King\" and \"Queen\" are similar**?\n",
    "This means the computer understands their relationship!\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎓 Coolest Trick of Word Embeddings\n",
    "Mathematically, word embeddings can solve word puzzles!\n",
    "\n",
    "👉 **\"King\" - \"Man\" + \"Woman\" = \"Queen\"**\n",
    "\n",
    "(If you remove \"man\" from \"king\" and add \"woman\", you get \"queen\")\n",
    "\n",
    "🤯 That’s because words store real meanings and relationships!\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔥 Popular Word Embeddings\n",
    "✅ **Word2Vec** – Learns words by reading millions of sentences 📖\n",
    "✅ **GloVe** – Builds word meanings from word co-occurrences 🤝\n",
    "✅ **FastText** – Understands parts of words (so \"playing\" and \"played\" are similar) 🎮\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚖️ How This Differs From Traditional Methods\n",
    "###### 🔵 One-Hot Encoding\n",
    "- Each word is represented by a long vector full of 0s and a single 1.\n",
    "- **Problem:** No similarity between words (\"cat\" and \"dog\" are as different as \"cat\" and \"car\").\n",
    "\n",
    "###### 🟡 Bag of Words (BoW)\n",
    "- Words are counted in a document, ignoring order and context.\n",
    "- **Problem:** It treats all words as independent, missing relationships between them.\n",
    "\n",
    "###### 🟠 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Weights words by importance in a document.\n",
    "- **Problem:** Still lacks an understanding of meaning and similarity between words.\n",
    "\n",
    "###### ✅ Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "- Words are represented as **dense numerical vectors** that capture meaning.\n",
    "- Similar words are **closer together in space**.\n",
    "- **Advantage:** Captures relationships like *\"King\" - \"Man\" + \"Woman\" = \"Queen\"*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 Final Takeaway\n",
    "- 📌 **Word embeddings are like a map of word meanings!**\n",
    "- 📌 **They group similar words together and help computers understand language!**\n",
    "- 📌 **That’s why AI can write, translate, and even chat like a human! 🤖**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Word Embeddings Mind Map  \n",
    "\n",
    "### 📌 Word Embeddings  \n",
    "- **Concept**: Words represented as vectors in a multi-dimensional space  \n",
    "  - Words with similar meanings are closer  \n",
    "  - Captures semantic relationships  \n",
    "  - Enables mathematical word analogies (e.g., **\"King\" - \"Man\" + \"Woman\" = \"Queen\"**)  \n",
    "\n",
    "### 🔢 Techniques  \n",
    "- **Word2Vec** (CBOW, Skip-gram)  \n",
    "- **GloVe** (Global Vectors)  \n",
    "- **FastText** (Character-based)  \n",
    "- **Transformer-based embeddings** (BERT, GPT)  \n",
    "\n",
    "### 🏠 Comparison with Traditional Methods  \n",
    "- **One-Hot Encoding**  \n",
    "  - Sparse representation  \n",
    "  - No similarity between words  \n",
    "\n",
    "- **Bag of Words (BoW)**  \n",
    "  - Counts word occurrences  \n",
    "  - Ignores word meaning & order  \n",
    "\n",
    "- **TF-IDF**  \n",
    "  - Weighs words by importance  \n",
    "  - Lacks semantic understanding  \n",
    "\n",
    "### 🔥 Applications  \n",
    "- **Sentiment Analysis**  \n",
    "- **Machine Translation**  \n",
    "- **Chatbots & Virtual Assistants**  \n",
    "- **Search Engines & Recommendation Systems**  \n",
    "\n",
    "### 📈 Advancements  \n",
    "- **Contextual embeddings** (ELMo, BERT)  \n",
    "- **Multilingual embeddings**  \n",
    "- **Zero-shot & Few-shot learning**  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

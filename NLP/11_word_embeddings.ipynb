{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– Word Embeddings Explained (Like You're 10 Years Old!)\n",
    "\n",
    "#### ğŸ­ Imagine Words as Characters in a Movie\n",
    "Think of each word as a character in a big Hollywood movie.\n",
    "Some characters are best friends, some are enemies, and some never meet.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **Superman and Batman** â†’ Similar (both superheroes ğŸ¦¸â€â™‚ï¸)\n",
    "- **Superman and Joker** â†’ Not similar (hero vs. villain ğŸ˜ˆ)\n",
    "- **Superman and Pizza** â†’ No real connection ğŸ•ğŸ¤”\n",
    "\n",
    "Just like characters have relationships, words also have relationships!\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ  How Words Were Stored Before (Bag of Words)\n",
    "Before word embeddings, computers stored words in a big table with just numbers.\n",
    "\n",
    "###### Example:\n",
    "| Sentence | Movie | Bad | Amazing | Loved | Acting |\n",
    "|----------|-------|-----|---------|-------|--------|\n",
    "| \"The movie was amazing\" | 1 | 0 | 1 | 0 | 0 |\n",
    "| \"I loved the acting\" | 0 | 0 | 0 | 1 | 1 |\n",
    "\n",
    "###### ğŸ‘ Problems:\n",
    "- It doesn't understand meaning (e.g., \"good\" and \"amazing\" are similar but treated as different words).\n",
    "- It only counts words but doesnâ€™t know if \"bad\" and \"terrible\" are related.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  How Word Embeddings Fix This\n",
    "\n",
    "###### ğŸŒ Words Live in a \"Word Universe\"\n",
    "Instead of just counting words, we place each word in a giant space (like a galaxy ğŸŒŒ) where:\n",
    "\n",
    "- **Similar words are closer together** ğŸš€\n",
    "- **Different words are far apart** ğŸï¸\n",
    "\n",
    "###### Example:\n",
    "- ğŸŸ¢ (\"King\" and \"Queen\") â†’ Very close ğŸ‘‘\n",
    "- ğŸ”µ (\"Dog\" and \"Cat\") â†’ Also close ğŸ¶ğŸ±\n",
    "- ğŸ”´ (\"King\" and \"Apple\") â†’ Very far apart ğŸ‘‘ğŸ\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¢ Numbers Behind the Magic (Vector Representation)\n",
    "Each word is stored as a list of numbers (called a \"vector\").\n",
    "\n",
    "###### Example:\n",
    "- **\"King\"** â†’ `[0.9, 0.8, 0.2, 0.1]`\n",
    "- **\"Queen\"** â†’ `[0.91, 0.79, 0.21, 0.09]`\n",
    "- **\"Apple\"** â†’ `[0.2, 0.1, 0.9, 0.8]`\n",
    "\n",
    "ğŸ‘€ Notice how **\"King\" and \"Queen\" are similar**?\n",
    "This means the computer understands their relationship!\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ Coolest Trick of Word Embeddings\n",
    "Mathematically, word embeddings can solve word puzzles!\n",
    "\n",
    "ğŸ‘‰ **\"King\" - \"Man\" + \"Woman\" = \"Queen\"**\n",
    "\n",
    "(If you remove \"man\" from \"king\" and add \"woman\", you get \"queen\")\n",
    "\n",
    "ğŸ¤¯ Thatâ€™s because words store real meanings and relationships!\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¥ Popular Word Embeddings\n",
    "âœ… **Word2Vec** â€“ Learns words by reading millions of sentences ğŸ“–\n",
    "âœ… **GloVe** â€“ Builds word meanings from word co-occurrences ğŸ¤\n",
    "âœ… **FastText** â€“ Understands parts of words (so \"playing\" and \"played\" are similar) ğŸ®\n",
    "\n",
    "---\n",
    "\n",
    "#### âš–ï¸ How This Differs From Traditional Methods\n",
    "###### ğŸ”µ One-Hot Encoding\n",
    "- Each word is represented by a long vector full of 0s and a single 1.\n",
    "- **Problem:** No similarity between words (\"cat\" and \"dog\" are as different as \"cat\" and \"car\").\n",
    "\n",
    "###### ğŸŸ¡ Bag of Words (BoW)\n",
    "- Words are counted in a document, ignoring order and context.\n",
    "- **Problem:** It treats all words as independent, missing relationships between them.\n",
    "\n",
    "###### ğŸŸ  TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Weights words by importance in a document.\n",
    "- **Problem:** Still lacks an understanding of meaning and similarity between words.\n",
    "\n",
    "###### âœ… Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "- Words are represented as **dense numerical vectors** that capture meaning.\n",
    "- Similar words are **closer together in space**.\n",
    "- **Advantage:** Captures relationships like *\"King\" - \"Man\" + \"Woman\" = \"Queen\"*.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ¯ Final Takeaway\n",
    "- ğŸ“Œ **Word embeddings are like a map of word meanings!**\n",
    "- ğŸ“Œ **They group similar words together and help computers understand language!**\n",
    "- ğŸ“Œ **Thatâ€™s why AI can write, translate, and even chat like a human! ğŸ¤–**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ Word Embeddings Mind Map  \n",
    "\n",
    "### ğŸ“Œ Word Embeddings  \n",
    "- **Concept**: Words represented as vectors in a multi-dimensional space  \n",
    "  - Words with similar meanings are closer  \n",
    "  - Captures semantic relationships  \n",
    "  - Enables mathematical word analogies (e.g., **\"King\" - \"Man\" + \"Woman\" = \"Queen\"**)  \n",
    "\n",
    "### ğŸ”¢ Techniques  \n",
    "- **Word2Vec** (CBOW, Skip-gram)  \n",
    "- **GloVe** (Global Vectors)  \n",
    "- **FastText** (Character-based)  \n",
    "- **Transformer-based embeddings** (BERT, GPT)  \n",
    "\n",
    "### ğŸ  Comparison with Traditional Methods  \n",
    "- **One-Hot Encoding**  \n",
    "  - Sparse representation  \n",
    "  - No similarity between words  \n",
    "\n",
    "- **Bag of Words (BoW)**  \n",
    "  - Counts word occurrences  \n",
    "  - Ignores word meaning & order  \n",
    "\n",
    "- **TF-IDF**  \n",
    "  - Weighs words by importance  \n",
    "  - Lacks semantic understanding  \n",
    "\n",
    "### ğŸ”¥ Applications  \n",
    "- **Sentiment Analysis**  \n",
    "- **Machine Translation**  \n",
    "- **Chatbots & Virtual Assistants**  \n",
    "- **Search Engines & Recommendation Systems**  \n",
    "\n",
    "### ğŸ“ˆ Advancements  \n",
    "- **Contextual embeddings** (ELMo, BERT)  \n",
    "- **Multilingual embeddings**  \n",
    "- **Zero-shot & Few-shot learning**  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
